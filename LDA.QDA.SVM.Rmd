---
title: 'Classification: LDA, QDA, and SVM'
author: "Chase Sinclair"
date: "March 25, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Classification 
If the response variable in a supervized learning problem is categorical, that means we have to carry out classification. Here a categorical variable can mean any non-numerical, unordered variable. For example, suppose the response variable is color, and can take three values: red, blue, and green. This is a non-numerical, unordered variable.

First, clear the workspace, and then install (if needed) and load the required libraries.

```{R 1}
rm(list=ls())
library("MASS")
library("ggplot2")
library("gridExtra")
```

## Iris data
We will work with the famous Iris dataset. This is one of the most well-known and classical datasets in all of statistics. Let's load the data and look into the help file to learn more about the context.


```{R 2}
data.iris = iris
help(iris)
```

Our goal is to construct a supervized learning model which can predict the species of a flower. Our response variable of interest is "species". Since species is a categorical variable, this is a classification problem. From the help file, we know that species takes three different values. Note that this is already somewhat different from the logistic regression problem we had looked at, where there were two classes (default and non-default).

There are four features or X-variables, namely sepal length, sepal width, petal length, and petal width.



## Visualization 
Let's look at the data.


```{R 3}
p1 = ggplot(data.iris,aes(x=Sepal.Length,y=Sepal.Width, colour = Species))+geom_point()
p2 = ggplot(data.iris,aes(x=Petal.Length,y=Petal.Width, colour = Species))+geom_point()
grid.arrange(ncol=2,nrow=1,p1,p2)
```
These two plots tell us that the three species are more clearly separated with respect to petal length and width, than sepal length and width. This is important to know as our goal is to predict species using sepal and petal dimensions.

## Classification using LDA

We will use Linear Discriminant Analysis (LDA) for classification. First let us use the sepal dimensions only.

```{R 4}
lda1 = lda(Species~Sepal.Length+Sepal.Width,data=data.iris)
```
How well does this work? To quantify the mis-classification error, we can compare the predicted species to the actual species.

```{R 5}
pred.lda1 = predict(lda1,data.iris)
table(pred.lda1$class,data.iris$Species)
sum(data.iris$Species!=pred.lda1$class)/nrow(data.iris)
```
The table gives us the cross-frequencey of predicted vs actual species. From the table we can see that 30 flowers are being mis-classified. The last line of the code patch calculates the fraction of cases where LDA makes an error, i.e., predicted species is different from actual species. Thus, the overall error rate is 20%. Note that this is not cross-validation error.

How good is 20%? Suppose we were guessing the species randomly. Since there are three species, we will make an error 2 out of 3 times, generally speaking, which implies an error rate of 66.7%. LDA using sepal length and width is doing much better than that. However, we might be able to do even better using other variables.


Now, suppose we were only using petal length and width.

```{R 6}
lda2 = lda(Species~Petal.Length+Petal.Width,data=data.iris)
pred.lda2 = predict(lda2,data.iris)
table(pred.lda2$class,data.iris$Species)
sum(data.iris$Species!=pred.lda2$class)/nrow(data.iris)
```

This model is doing much better than LDA1, as only 6 flowers are mis-classified and the error rate is 4%.

let's use both sepal and petal dimensions.

```{R 7}
lda3 = lda(Species~.,data=data.iris)
pred.lda3 = predict(lda3,data.iris)
table(pred.lda3$class,data.iris$Species)
sum(data.iris$Species!=pred.lda3$class)/nrow(data.iris)
```

This model is doing even better, with only 3 flowers mis-classified, an error rate of 2%.

## Classification using QDA

Next, we will apply QDA, or Quadratic Discriminant Analysis. As before, let's start with sepal dimensions only.

```{R 8}
qda1 = qda(Species~Sepal.Length+Sepal.Width,data=data.iris)
pred.qda1 = predict(qda1,data.iris)
table(pred.qda1$class,data.iris$Species)
sum(data.iris$Species!=pred.qda1$class)/nrow(data.iris)
```

The result is quite similar to LDA1, we have 30 flowers mis-classified, an error rate of 20%.

Next, let's try petal dimensions only.

```{R 9}
qda2 = qda(Species~Petal.Length+Petal.Width,data=data.iris)
pred.qda2 = predict(qda2,data.iris)
table(pred.qda2$class,data.iris$Species)
sum(data.iris$Species!=pred.qda2$class)/nrow(data.iris)
```

As with LDA, petal dimensions work better than sepal dimensions for QDA too. Here, we have 3 flowers mis-classified, an error rate of 2%.

Next, let's try both sepal and petal dimensions.

```{R 10}
qda3 = qda(Species~.,data=data.iris)
pred.qda3 = predict(qda3,data.iris)
table(pred.qda3$class,data.iris$Species)
sum(data.iris$Species!=pred.qda3$class)/nrow(data.iris)
```

The result is quite similar to LDA3, we have 3 flowers mis-classified, an error rate of 2%.

Next, let's try SVM.

## Classification using QDA

As before, let's start with sepal dimensions only.

```{R 11}
library(e1071) # You may need to install this package first
svm1 = svm(Species~Sepal.Length+Sepal.Width,data=data.iris)
pred.svm1 = predict(svm1,data.iris)
table(pred.svm1,data.iris$Species) # note the difference in syntax
sum(data.iris$Species!=pred.svm1)/nrow(data.iris)
```

The result is similar to LDA and QDA, we get 27 flowers mis-classified, with an error rate of 18%.

Next, let's try petal dimensions only.

```{R 12}
svm2 = svm(Species~Petal.Length+Petal.Width,data=data.iris)
pred.svm2 = predict(svm2,data.iris)
table(pred.svm2,data.iris$Species) # note the difference in syntax
sum(data.iris$Species!=pred.svm2)/nrow(data.iris)
```
The result is similar to LDA and QDA, we get 6 flowers mis-classified, with an error rate of 4%.

Next, let's try both sepal and petal dimensions.

```{R 13}
svm3 = svm(Species~.,data=data.iris)
pred.svm3 = predict(svm3,data.iris)
table(pred.svm3,data.iris$Species) # note the difference in syntax
sum(data.iris$Species!=pred.svm3)/nrow(data.iris)
```


The result is similar to LDA and QDA, we get 4 flowers mis-classified, with an error rate of 2.67%.

Next, let's look at cross-validation errors.

## Cross-validation error

A central goal of classification is to predict the values of the response variable. For the iris data, suppose we know the sepal and petal length and width of a flower, and we want to predict the species of this flower.

Therefore, an important question is: how good is our model for making predictions? We can quantify this by partitioning the available data into two sets, the training set and the testing set. Data from the training set is used to fit the model, and then we use this model to predict the response variable for the test data. By comparing the predicted values and the true values for the test data, we can quantify the accuracy of the model. This idea is called cross-validation.

First, let's split the data randomly into training and test sets. We will use around 70% of the data for training. We will use all four variables for classification.

```{R 14}
n = nrow(data.iris)
train.prop = 0.7
train.size = ceiling(n*train.prop)
test.size = n-train.size

foo = sample(n,train.size)
train.data = data.iris[foo,]
test.data = data.iris[-foo,]
lda.cv = lda(Species~.,data=train.data)
qda.cv = qda(Species~.,data=train.data)
svm.cv = svm(Species~.,data=train.data)

pred.lda = predict(lda.cv, test.data)
pred.qda = predict(qda.cv, test.data)
pred.svm = predict(svm.cv, test.data)

lda.err.cv = sum(test.data$Species!=pred.lda$class)/nrow(test.data)
qda.err.cv = sum(test.data$Species!=pred.qda$class)/nrow(test.data)
svm.err.cv = sum(test.data$Species!=pred.svm)/nrow(test.data)

lda.err.cv; qda.err.cv; svm.err.cv
```
  
The results that you see above are very likely different from the results that you get when you run the code yourself. This happens because we are randomly partitioning the data into training and test, which means we get a new partition every time this is done. As the results depend on the training and test data, they will vary. This makes the results somewhat volatile.

To resolve this issue, we should carry out cross-validation for a large number of iterations, and take the average error across these iterations. The following patch does this for 1000 iterations.

```{R 15}
K = 1000
for (i in 1:K){
  foo = sample(n,train.size)
  train.data = data.iris[foo,]
  test.data = data.iris[-foo,]
  lda.cv = lda(Species~.,data=train.data)
  qda.cv = qda(Species~.,data=train.data)
  svm.cv = svm(Species~.,data=train.data)
  
  pred.lda = predict(lda.cv, test.data)
  pred.qda = predict(qda.cv, test.data)
  pred.svm = predict(svm.cv, test.data)
  
  lda.err.cv[i] = sum(test.data$Species!=pred.lda$class)/nrow(test.data)
  qda.err.cv[i] = sum(test.data$Species!=pred.qda$class)/nrow(test.data)
  svm.err.cv[i] = sum(test.data$Species!=pred.svm)/nrow(test.data)
}
round(mean(lda.err.cv),3);round(mean(qda.err.cv),3);round(mean(svm.err.cv),3)
```

We observe that while the errors are small for all three methods, LDA performs the best (lowest average error), followed by QDA, and then SVM.


